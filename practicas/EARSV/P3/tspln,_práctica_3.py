# -*- coding: utf-8 -*-
"""TSPLN, Práctica 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vdxTVEkddWJSK5piiVlTfXTIrFNtolE8
"""

!pip install spacy datasets
!python -m spacy download es_core_news_md

#En este punto se requiere reiniciar la sesión para que no dé error.
!pip install gensim

from collections import Counter, defaultdict
from datasets import load_dataset
import matplotlib.pyplot as plt
from itertools import combinations
import numpy as np
import spacy
from spacy.tokens import Doc

from sklearn.decomposition import PCA, TruncatedSVD
import random
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

"""# 3. Práctica: Vectores a palabras

**Fecha de entrega: 16 de Marzo de 2025 @ 11:59pm**

Obtenga la matriz de co-ocurrencia para un corpus en español y realice los siguientes calculos:
- Las probabilidades conjuntas
$$p(w_i,w_j) = \frac{c_{i,j}}{\sum_i \sum_j c_{i,j}}$$
- Las probabilidades marginales
$$p(w_i) = \sum_j p(w_i,w_j)$$
- Positive Point Wise Mutual Information (PPMI):
$$PPMI(w_i,w_j) = \max\{0, \log_2 \frac{p(w_i,w_j)}{p(w_i)p(w_j)}\}$$

**Comparación de representaciones**

Aplica reducción de dimensionalidad (a 2D) de los vectores de la matríz con PPMI y de los vectores entrenados en español:

- Realiza un plot de 100 vectores aleatorios (que esten tanto en la matríz como en los vectores entrenados)
- Compara los resultados de los plots:
    - ¿Qué representación dirías que captura mejor relaciones semánticas?
    - Realiza un cuadro comparativo de ambos métodos con ventajas/desventajas

### 📁 [Carpeta con vectores](https://drive.google.com/drive/folders/1reor2FGsfOB6m3AvfCE16NOHltAFjuvz?usp=drive_link)

#Matrix de Co-ocurrencia.
"""

ds_oscar = load_dataset("djstrong/oscar-small", "unshuffled_deduplicated_es")
ds_oscar=ds_oscar["train"]

nlp = spacy.load("es_core_news_md")

# Solo tokenización y lematización
def preprocesar(texto: str) -> list:
    doc = nlp(texto)
    tokens=[]
    for token in doc:
        if not token.is_stop and token.is_alpha and len(token.lemma_) > 2:
            tokens.append(token.lemma_.lower())
    tokens = [
        token.lemma_.lower()
        for token in doc
        if not token.is_stop
        and token.is_alpha
        and len(token.lemma_) > 2
    ]
    return tokens

docs_procesados = [preprocesar(texto) for texto in ds_oscar[:5000]["text"]]  # 5K documentos para prueba

print(docs_procesados[0])

# Contar frecuencias
frecuencias = defaultdict(int)
for doc in docs_procesados:
    for palabra in doc:
        frecuencias[palabra] += 1

# Vocabulario reducido (top 3000 palabras)
vocabulario = sorted(frecuencias, key=frecuencias.get, reverse=True)[:3000]
word2idx = {word: idx for idx, word in enumerate(vocabulario)}
vocab_size = len(vocabulario)

# Inicializar matriz
window_size = 5
co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)

# Llenar matriz
for doc in docs_procesados:
    for i, palabra in enumerate(doc):
        if palabra not in word2idx:
            continue
        idx_palabra = word2idx[palabra]
        # Ventana de contexto
        start = max(0, i - window_size)
        end = min(len(doc), i + window_size + 1)
        for j in range(start, end):
            if j == i:
                continue
            contexto = doc[j]
            if contexto in word2idx:
                idx_contexto = word2idx[contexto]
                co_matrix[idx_palabra, idx_contexto] += 1

# Probabilidad conjunta
total = co_matrix.sum().astype(float)
p_conjunta = co_matrix / total

# Marginales
p_marginal_i = p_conjunta.sum(axis=1)
p_marginal_j = p_conjunta.sum(axis=0)

# PPMI
eps = 1e-8
pmi = np.log2((p_conjunta + eps) / (np.outer(p_marginal_i, p_marginal_j) + eps))
ppmi = np.maximum(pmi, 0)

if "artificial" in word2idx:
    idx = word2idx["artificial"]
    print(f"Palabra: 'artificial'")
    print(f"Co-ocurrencias totales: {co_matrix[idx].sum()}")
    print(f"PPMI máximo: {ppmi[idx].max():.2f}")

# Seleccionar top N palabras para visualización
top_n = 20
top_words = vocabulario[:top_n]
indices = [word2idx[word] for word in top_words]

# Submatriz reducida
submatrix = np.log1p(co_matrix[indices][:, indices])  # log(1 + x) para suavizar

# Configurar plot
plt.figure(figsize=(12, 10))
sns.heatmap(
    submatrix,
    annot=True,
    fmt=".1f",
    cmap="viridis",
    xticklabels=top_words,
    yticklabels=top_words,
)
plt.title("Log-Co-ocurrencias (Top 20 palabras)")
plt.xticks(rotation=45, ha="right")
plt.show()

# Obtener pares y sus conteos
pares = []
for i in range(vocab_size):
    for j in range(vocab_size):
        if co_matrix[i, j] > 0:
            pares.append((vocabulario[i], vocabulario[j], co_matrix[i, j]))

# Ordenar y seleccionar top 10
top_pares = sorted(pares, key=lambda x: x[2], reverse=True)[:10]
palabras1, palabras2, conteos = zip(*top_pares)

# Gráfico de barras
plt.figure(figsize=(10, 6))
plt.barh(
    [f"{p1} - {p2}" for p1, p2 in zip(palabras1, palabras2)],
    conteos,
    color="skyblue",
)
plt.gca().invert_yaxis()
plt.xlabel("Frecuencia de co-ocurrencia")
plt.title("Top 10 pares de palabras co-ocurrentes")
plt.show()

# Reducción a 2D
pca = PCA(n_components=2)
coords = pca.fit_transform(ppmi)

# Plot
plt.figure(figsize=(12, 8))
plt.scatter(coords[:, 0], coords[:, 1], alpha=0.5)
for i, word in enumerate(vocabulario[:50]):  # Primeras 50 palabras
    plt.annotate(word, (coords[i, 0], coords[i, 1]))
plt.title("Embeddings PPMI reducidos con PCA")
plt.show()

"""#Entrenamiento con word2vec"""

from gensim.models import word2vec
def load_model(model_path: str):
    try:
        print(model_path)
        return word2vec.Word2Vec.load(model_path)
    except:
        print(f"[WARN] Model not found in path {model_path}")
        return None

def report_stats(model) -> None:
    """Print report of a model"""
    print("Number of words in the corpus used for training the model: ", model.corpus_count)
    print("Number of words in the model: ", len(model.wv.index_to_key))
    print("Time [s], required for training the model: ", model.total_train_time)
    print("Count of trainings performed to generate this model: ", model.train_count)
    print("Length of the word2vec vectors: ", model.vector_size)
    print("Applied context length for generating the model: ", model.window)

from enum import Enum

class Algorithms(Enum):
    CBOW = 0
    SKIP_GRAM = 1

def train_model(sentences: list, model_name: str, vector_size: int, window=5, workers=2, algorithm = Algorithms.CBOW):
    model_name_params = f"{model_name}-vs{vector_size}-w{window}-{algorithm.name}.model"
    model_path = MODELS_DIR + model_name_params
    if load_model(model_path) is not None:
        print(f"Already exists the model {model_path}")
        return load_model(model_path)
    print(f"TRAINING: {model_path}")
    if algorithm in [Algorithms.CBOW, Algorithms.SKIP_GRAM]:
        print(f"Algorithm: {algorithm.name}")
        model = word2vec.Word2Vec(
            sentences,
            vector_size=vector_size,
            window=window,
            workers=workers,
            sg = algorithm.value,
            seed=42,
            )
    else:
        print("[ERROR] algorithm not implemented yet :p")
        return
    try:
        model.save(model_path)
    except:
        print(f"[ERROR] Saving model at {model_path}")
    return model

MODELS_DIR='/content/drive/MyDrive/Ciencia de Datos/8.Octavo Semestre/Temas selectos de PLN/Pŕacticas/Práctica 3/models/word2vec/'
#print(MODELS_DIR)
#model_name = "eswiki-md-300-CBOW.model"
#model = load_model(MODELS_DIR + model_name)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# cbow_model = train_model(
#     docs_procesados,
#     "oscar",
#     vector_size=300,
#     window=5,
#     workers=2,
#     algorithm=Algorithms.CBOW
# )

report_stats(cbow_model)

cbow_model.wv["inteligencia"]

"""#Comparación entre Word2Vec y PPMI"""

# Palabras del modelo Word2Vec
words_word2vec = set(cbow_model.wv.key_to_index.keys())

# Palabras de la matriz PPMI
words_ppmi = set(vocabulario)

# Intersección de vocabularios
common_words = list(words_word2vec & words_ppmi)
print(f"Palabras comunes: {len(common_words)}")

# Seleccionar 100 palabras aleatorias
random.seed(42)
selected_words = random.sample(common_words, 100)

# Vectores Word2Vec
word2vec_vectors = np.array([cbow_model.wv[word] for word in selected_words])

# Vectores PPMI (usar los índices de la matriz)
ppmi_indices = [word2idx[word] for word in selected_words]
ppmi_vectors = ppmi[ppmi_indices]

# Se usa PCA para reducir dimensionalidad
pca = PCA(n_components=2)
word2vec_2d = pca.fit_transform(word2vec_vectors)

ppmi_2d = pca.fit_transform(ppmi_vectors)

plt.figure(figsize=(14, 7))

# Subplot para Word2Vec
plt.subplot(1, 2, 1)
plt.scatter(word2vec_2d[:, 0], word2vec_2d[:, 1], alpha=0.7, c='blue')
plt.title('Word2Vec (PCA)')
plt.grid(True, linestyle='--', alpha=0.5)

# Anotar algunas palabras clave
for i, word in enumerate(selected_words[:10]):  # Primeras 10 para claridad
    plt.annotate(word, (word2vec_2d[i, 0], word2vec_2d[i, 1]), fontsize=8)

# Subplot para PPMI
plt.subplot(1, 2, 2)
plt.scatter(ppmi_2d[:, 0], ppmi_2d[:, 1], alpha=0.7, c='red')
plt.title('PPMI (PCA)')
plt.grid(True, linestyle='--', alpha=0.5)

# Anotar mismas palabras para comparar
for i, word in enumerate(selected_words[:10]):
    plt.annotate(word, (ppmi_2d[i, 0], ppmi_2d[i, 1]), fontsize=8)

plt.tight_layout()
plt.show()

"""Lo que podemos observar es que en el caso de Word2Vec el rango de valores es mucho menor que en el caso de PPMI, pues como podemos observar, los valores en x van desde -4 hasta 1 para x y de -0.01 a 0.2 para y, por otro lado, para PPMI los valores van desde -10 hasta 25 en x y desde -10 hasta 25 para y, lo que nos habla de una mayor distancia entre los vectores. Al tratarse de las mismas palabras las relaciones semánticas deberían de ser similares y si ciertas palabras están relacionadas PPMI también debería de captar esto, sin embargo, podemos notar que hay una mayor dispersión para el caso de PPMI, lo que nos habla de que es peor a la hora de captar relaciones semánticas. De este modo, la representación vectorial mediante Word2Vec es mejor para captar relaciones semánticas.


| Característica          | Word2Vec              | PPMI                  |
|-------------------------|-----------------------|-----------------------|
| Relaciones semánticas    | Captura mejor         | Captura parcial       |
| Densidad informativa     | Alta (vectores densos)| Baja (matriz dispersa)|
| Interpretabilidad visual | Buena                 | Limitada              |
| Contextos locales        | Modela bien           | Modela explícitamente |
| Palabras raras           | Manejo regular        | Pobre manejo          |




"""