# -*- coding: utf-8 -*-
"""TSPLN, práctica 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nFpKcNKBoj_uq7AnBq4QLcTN7a3R-4Ug
"""

#Importamos librerías
import torch
from torch import nn
import nltk
from nltk.corpus import reuters
from nltk.corpus import stopwords
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import string
from sklearn.metrics.pairwise import cosine_similarity

#Descargamos corpus y herramientas de procesamiento
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('reuters')
nltk.download('punkt')

#Montamos nuestro drive
from google.colab import drive
drive.mount('/content/drive')

# Copiamos código visto en clase necesario para que todo funcione correctamente
def preprocess_corpus(corpus: list[str]) -> list[str]:
    preprocessed_corpus = []
    for sent in corpus:
        result = [word.lower() for word in sent]
        result.append("<EOS>")
        result.insert(0, "<BOS>")
        preprocessed_corpus.append(result)
    return preprocessed_corpus

def get_words_freqs(corpus: list[list[str]]):
    words_freqs = {}
    for sentence in corpus:
        for word in sentence:
            words_freqs[word] = words_freqs.get(word, 0) + 1
    return words_freqs

def get_words_indexes(words_freqs: dict) -> dict:
    result = {}
    for idx, word in enumerate(words_freqs.keys()):
        if words_freqs[word] == 1:
            result[UNK_LABEL] = len(words_freqs)
        else:
            result[word] = idx
    return {word: idx for idx, word in enumerate(result.keys())}, {idx: word for idx, word in enumerate(result.keys())}

# Trigram Neural Network Model
class TrigramModel(nn.Module):
    """Clase padre: https://pytorch.org/docs/stable/generated/torch.nn.Module.html"""

    def __init__(self, vocab_size, embedding_dim, context_size, h):
        super(TrigramModel, self).__init__()
        self.context_size = context_size
        self.embedding_dim = embedding_dim
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear1 = nn.Linear(context_size * embedding_dim, h)
        self.linear2 = nn.Linear(h, vocab_size)

    def forward(self, inputs):
        # x': concatenation of x1 and x2 embeddings   -->
        # self.embeddings regresa un vector por cada uno de los índices que se les pase como entrada.
        # view() les cambia el tamaño para concatenarlos
        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))
        # h: tanh(W_1.x' + b)  -->
        out = torch.tanh(self.linear1(embeds))
        # W_2.h                 -->
        out = self.linear2(out)
        # log_softmax(W_2.h)      -->
        # dim=1 para que opere sobre renglones, pues al usar batchs tenemos varios vectores de salida
        log_probs = F.log_softmax(out, dim=1)

        return log_probs

#Realizamos una pequeña configuración para que el load funcionara bien con el dispositivo actual
def get_model(path: str, device='cpu') -> TrigramModel:
    """Obtiene modelo de pytorch desde disco"""
    model_loaded = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)
    model_loaded.load_state_dict(torch.load(path,map_location=device))
    model_loaded.eval()
    return model_loaded

#Creamos una función para obtener las n palabras más similares a una palabra dada mediante la similitud coseno
def topn_sim_words(word,word_embeddings, n=10):
    """Obtiene las n palabras más similares a una dada mediante la similitud coseno"""
    similitudes={}
    #print(similitudes)
    if word in word_embeddings:
     embedding_word= word_embeddings[word]
     #print(embedding_word)
     for words,embedding in word_embeddings.items():
        if words==word:
            continue
        words_embedding= embedding

        similitudes[words]=cosine_similarity(embedding_word.reshape(1,-1),words_embedding.reshape(1,-1))[0][0]
     #print(similitudes)
     similitudes_ord=sorted(similitudes.items(),key=lambda x:x[1],reverse=True)
     return similitudes_ord[:n]
    else:
     return None

"""Obtenemos los embeddings dados por el modelo"""

#Parámetros vistos en clase
EMBEDDING_DIM = 200
CONTEXT_SIZE = 2
H = 100
UNK_LABEL = "<UNK>"

# Cargamos datos y creamos vocabulario
corpus = preprocess_corpus(reuters.sents())
words_freqs = get_words_freqs(corpus)
words_indexes, index_to_word = get_words_indexes(words_freqs)
V = len(words_indexes)

# Cargamos modelo entrenado en clase
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_path = "/content/drive/MyDrive/Ciencia de Datos/8.Octavo Semestre/Temas selectos de PLN/Pŕacticas/Práctica 4/Models/model_cuda_context_2_epoch_9.dat"
model= get_model(model_path,device)


# Extraemos embeddings
embeddings = model.embeddings.weight.data

# Creamos diccionario de embeddings
word_embeddings = {word: embeddings[idx].numpy()
                  for word, idx in words_indexes.items()
                  if idx < embeddings.shape[0]}

# Probamos nuestro método con la palabra the
print("Embedding para 'the':", word_embeddings.get('the', 'Palabra no encontrada'))
print("Dimensión del embedding:", embeddings.shape[1])

"""Visualizamos los embeddings en 2D mediante reducción de dimensionalidad de las palabras más frecuentes excluyendo stopwords, tokens especiales y signos de puntuación."""

#Obtener las stopwords
stop_words = set(stopwords.words('english'))

# Obtuvimos las 50 palabras más frecuentes (excluyendo stopwords, tokens especiales y símbolos de puntuación)
top_words = sorted([(freq, word) for word, freq in words_freqs.items()
                   if word not in stop_words
                   and word not in ["<BOS>", "<EOS>", "<UNK>"] and word not in string.punctuation], reverse=True)[:100]

# Obtuvimos los embeddings correspondientes
selected_words = [word for _, word in top_words]
selected_embeddings = [embeddings[words_indexes[word]].numpy() for word in selected_words]

# Redujimos dimensionalidad con t-SNE para una mejor visualización
tsne = TSNE(n_components=2, random_state=42)
embeddings_2d = tsne.fit_transform(np.array(selected_embeddings))

# Configuramos el gráfico
plt.figure(figsize=(15, 10))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)

# Añadimos etiquetas
for i, word in enumerate(selected_words):
    plt.annotate(word,
                 (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                 textcoords="offset points",
                 xytext=(5,2),
                 ha='left')

plt.title("Visualización 2D de embeddings usando t-SNE")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.show()

"""Lo que podemos observar es que si bien es cierto algunas palabras están cercanas (como los años, meses o números) en general tenemos una alta dispersión, lo cual no es buena señal pues deberíamos de ver algunas agrupaciones en ciertos temas en común.

Obtenemos las 10 palabras más cercanas semánticamente mediante similitud coseno de algunas palabras de prueba.
"""

#Obtener similitud de diferentes palabras
words_sim=topn_sim_words('march',word_embeddings)
print(f'Palabras cercanas a March: {words_sim}')
words_sim=topn_sim_words('microsoft', word_embeddings)
print(f'Palabras cercanas a Microsoft: {words_sim}')
words_sim=topn_sim_words('government', word_embeddings)
print(f'Palabras cercanas a Government: {words_sim}')
words_sim=topn_sim_words('mexico', word_embeddings)
print(f'Palabras cercanas a Mexico: {words_sim}')
words_sim=topn_sim_words('car', word_embeddings)
print(f'Palabras cercanas a Car: {words_sim}')

"""Al obtener palabras semánticamente similares mediante la distancia coseno, podemos observar que no fuimos capaces de captar relaciones semánticas, pues en la mayoría de los casos no tiene mucho sentido las palabras con mayor similitud coseno. Además, podemos observar que los valores de la similitud no son tan altos, lo que nos habla de que si bien es cierto que son las palabras más cercanas no están tan cercanas como quisieramos. Por útlimo, atribuímos esto al corpus con el que fue entrenado nuestro modelo. Investigando un poco notamos que nuestro corpus (Reusters) está compuesto por noticias financieras y de otro tipo, lo que lo hace no tan diverso como otros corpus más amplios (como Wikipedia), además, el lenguaje suele ser específico del dominio periodístico y financiero, lo que podrías limitar la generalización de las relaciones semánticas. Normalmente, Reuters se utiliza para realizar clasificación de texto, pero no es común usarlo para capturar relaciones semánticas por todo lo comentado anteriormente. De esta manera, notamos la gran importancia de la correcta elección del corpus de entrenamiento para que nuestro modelo lleva a cabo ciertas tareas de manera eficiente.

Nota: Se intentó utilizar los modelos proporcionados mediante drive. Sin embargo, al tener un vocabulario menor no fuimos capaces de poder utilizarlos.
"""